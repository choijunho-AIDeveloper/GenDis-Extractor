{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìœ ì „ìž ë³€ì´ í•´ì„\n",
    "Get Data \"wget https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz\" to CMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   #AlleleID                       Type  \\\n",
      "0      15041                      Indel   \n",
      "1      15041                      Indel   \n",
      "2      15042                   Deletion   \n",
      "3      15042                   Deletion   \n",
      "4      15043  single nucleotide variant   \n",
      "\n",
      "                                                Name  GeneID GeneSymbol  \\\n",
      "0  NM_014855.3(AP5Z1):c.80_83delinsTGCTGTAAACTGTA...    9907      AP5Z1   \n",
      "1  NM_014855.3(AP5Z1):c.80_83delinsTGCTGTAAACTGTA...    9907      AP5Z1   \n",
      "2     NM_014855.3(AP5Z1):c.1413_1426del (p.Leu473fs)    9907      AP5Z1   \n",
      "3     NM_014855.3(AP5Z1):c.1413_1426del (p.Leu473fs)    9907      AP5Z1   \n",
      "4       NM_014630.3(ZNF592):c.3136G>A (p.Gly1046Arg)    9640     ZNF592   \n",
      "\n",
      "      HGNC_ID    ClinicalSignificance  ClinSigSimple LastEvaluated  \\\n",
      "0  HGNC:22197              Pathogenic              1  Jun 25, 2024   \n",
      "1  HGNC:22197              Pathogenic              1  Jun 25, 2024   \n",
      "2  HGNC:22197              Pathogenic              1  Jun 29, 2010   \n",
      "3  HGNC:22197              Pathogenic              1  Jun 29, 2010   \n",
      "4  HGNC:28986  Uncertain significance              0  Jun 29, 2015   \n",
      "\n",
      "   RS# (dbSNP)  ...      AlternateAlleleVCF SomaticClinicalImpact  \\\n",
      "0    397704705  ...  TGCTGTAAACTGTAACTGTAAA                     -   \n",
      "1    397704705  ...  TGCTGTAAACTGTAACTGTAAA                     -   \n",
      "2    397704709  ...                       G                     -   \n",
      "3    397704709  ...                       G                     -   \n",
      "4    150829393  ...                       A                     -   \n",
      "\n",
      "  SomaticClinicalImpactLastEvaluated ReviewStatusClinicalImpact Oncogenicity  \\\n",
      "0                                  -                          -            -   \n",
      "1                                  -                          -            -   \n",
      "2                                  -                          -            -   \n",
      "3                                  -                          -            -   \n",
      "4                                  -                          -            -   \n",
      "\n",
      "  OncogenicityLastEvaluated ReviewStatusOncogenicity  \\\n",
      "0                         -                        -   \n",
      "1                         -                        -   \n",
      "2                         -                        -   \n",
      "3                         -                        -   \n",
      "4                         -                        -   \n",
      "\n",
      "  SCVsForAggregateGermlineClassification  \\\n",
      "0              SCV001451119|SCV005622007   \n",
      "1              SCV001451119|SCV005622007   \n",
      "2                           SCV000020156   \n",
      "3                           SCV000020156   \n",
      "4                           SCV000020157   \n",
      "\n",
      "  SCVsForAggregateSomaticClinicalImpact  \\\n",
      "0                                     -   \n",
      "1                                     -   \n",
      "2                                     -   \n",
      "3                                     -   \n",
      "4                                     -   \n",
      "\n",
      "   SCVsForAggregateOncogenicityClassification  \n",
      "0                                           -  \n",
      "1                                           -  \n",
      "2                                           -  \n",
      "3                                           -  \n",
      "4                                           -  \n",
      "\n",
      "[5 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./variant_summary.csv\"\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Train Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mutations in the BTBD3 gene can cause See cases.', 'BTBD3', 'See cases')\n",
      "('Variations in the BTBD3 gene are associated with See cases.', 'BTBD3', 'See cases')\n",
      "('Abnormalities in the BTBD3 gene may contribute to See cases.', 'BTBD3', 'See cases')\n",
      "('Specific mutations in the BTBD3 gene can increase the risk of See cases.', 'BTBD3', 'See cases')\n",
      "('Certain variants of the BTBD3 gene are frequently observed in patients with See cases.', 'BTBD3', 'See cases')\n"
     ]
    }
   ],
   "source": [
    "# Train Data Generate\n",
    "sentences = []\n",
    "\n",
    "# Filter\n",
    "Filter = [\"not specified\", \"not provided\", \"unknown\"]\n",
    "\n",
    "# Template\n",
    "templates = [\n",
    "    \"Mutations in the {gene} gene can cause {disease}.\",\n",
    "    \"Variations in the {gene} gene are associated with {disease}.\",\n",
    "    \"Abnormalities in the {gene} gene may contribute to {disease}.\",\n",
    "    \"Specific mutations in the {gene} gene can increase the risk of {disease}.\",\n",
    "    \"Certain variants of the {gene} gene are frequently observed in patients with {disease}.\"\n",
    "    ]\n",
    "\n",
    "unique_gene_disease = set()\n",
    "for _, row in df.iterrows():\n",
    "    gene = row[\"GeneSymbol\"]\n",
    "    disease = row[\"PhenotypeList\"].split(\"|\")[0]  # \"Hereditary spastic paraplegia 48|not provided\" -> \"Hereditary spastic paraplegia 48\"\n",
    "\n",
    "    if disease.lower() not in Filter:\n",
    "        unique_gene_disease.add((gene, disease))\n",
    "\n",
    "for gene, disease in unique_gene_disease:\n",
    "    for template in templates:\n",
    "        sentence = template.format(gene = gene, disease = disease)\n",
    "        sentences.append((sentence, gene, disease))\n",
    "\n",
    "for sentence in sentences[:5]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio(sentence, gene, disease):\n",
    "    words = sentence.split()\n",
    "    labels = [\"O\"] * len(words)\n",
    "\n",
    "    # Gene Tagging\n",
    "    for i, word in enumerate(words):\n",
    "        if word == gene:\n",
    "            labels[i] = \"B-GENE\"\n",
    "    \n",
    "    # Disease Tagging\n",
    "    disease_words = disease.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word in disease_words:\n",
    "            labels[i] = \"B-DISEASE\" if i == 0 else \"I-DISEASE\"\n",
    "    \n",
    "    return list(zip(words, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mutations', 'O'), ('in', 'O'), ('the', 'O'), ('BTBD3', 'B-GENE'), ('gene', 'O'), ('can', 'O'), ('cause', 'O'), ('See', 'I-DISEASE'), ('cases.', 'O')]\n",
      "[('Variations', 'O'), ('in', 'O'), ('the', 'O'), ('BTBD3', 'B-GENE'), ('gene', 'O'), ('are', 'O'), ('associated', 'O'), ('with', 'O'), ('See', 'I-DISEASE'), ('cases.', 'O')]\n",
      "[('Abnormalities', 'O'), ('in', 'O'), ('the', 'O'), ('BTBD3', 'B-GENE'), ('gene', 'O'), ('may', 'O'), ('contribute', 'O'), ('to', 'O'), ('See', 'I-DISEASE'), ('cases.', 'O')]\n",
      "[('Specific', 'O'), ('mutations', 'O'), ('in', 'O'), ('the', 'O'), ('BTBD3', 'B-GENE'), ('gene', 'O'), ('can', 'O'), ('increase', 'O'), ('the', 'O'), ('risk', 'O'), ('of', 'O'), ('See', 'I-DISEASE'), ('cases.', 'O')]\n",
      "[('Certain', 'O'), ('variants', 'O'), ('of', 'O'), ('the', 'O'), ('BTBD3', 'B-GENE'), ('gene', 'O'), ('are', 'O'), ('frequently', 'O'), ('observed', 'O'), ('in', 'O'), ('patients', 'O'), ('with', 'O'), ('See', 'I-DISEASE'), ('cases.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "bio_sentences = [convert_to_bio(sentence, gene, disease) for sentence, gene, disease in sentences]\n",
    "\n",
    "for bio in bio_sentences[:5]:\n",
    "    print(bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER Train Data Save\n",
    "with open(\"ner_dataset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for bio in bio_sentences:\n",
    "        for word, tag in bio:\n",
    "            f.write(f\"{word} {tag}\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\"O\" : Nothing\n",
    "\"B-GENE\" : Gene Token(Begin)\n",
    "\"I-GENE\" : Gene Token(Inside)\n",
    "\"B-DISEASE\" : Disease Token(Begin)\n",
    "\"I-DISEASE\" : Disease Token(Inside)\n",
    "'''\n",
    "\n",
    "label_list = [\"O\", \"B-GENE\", \"I-GENE\", \"B-DISEASE\", \"I-DISEASE\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence combine\n",
    "def load_ner_data(file_path):\n",
    "    # Restore Sentence\n",
    "    sentences = []\n",
    "    # Words in Sentence\n",
    "    words = []\n",
    "    # Tag in Sentence\n",
    "    tags = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # config \"\"(Finish Sentence)\n",
    "            if not line:\n",
    "                if words:\n",
    "                    sentences.append((words, tags))\n",
    "                    words = []\n",
    "                    tags = []\n",
    "            else:\n",
    "                token, label = line.split()\n",
    "                words.append(token)\n",
    "                tags.append(label)\n",
    "        # If Last Sentence not Finish with \"\" Preprocess\n",
    "        if words:\n",
    "            sentences.append((words, tags))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./ner_dataset.txt\"\n",
    "dataset = load_ner_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Valid, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : valid :  test : 189108 : 24314 : 56733\n"
     ]
    }
   ],
   "source": [
    "# Split train : valid : test = 7 : 1 : 2\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.7, random_state=42)\n",
    "\n",
    "print(f\"train : valid :  test : {len(train_data)} : {len(valid_data)} : {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    words = examples['words']\n",
    "    tags = examples['tags']\n",
    "    \n",
    "    # Word to SubWord(Tokenizing)\n",
    "    tokenized = tokenizer(\n",
    "        words,\n",
    "        truncation=True,\n",
    "        padding = \"max_length\",\n",
    "        max_length = 128,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    # Subword Match Origin Word Label\n",
    "    label_ids = []\n",
    "    word_ids = tokenized.word_ids(batch_index=0)\n",
    "\n",
    "    previous_word_idx = None\n",
    "    previous_label = \"O\"\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        # If token [CLS], [SEP], [PAD], ignore\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        else:\n",
    "            current_label = tags[word_idx]\n",
    "            \n",
    "            # If Subword seperate GENE, label -> (B-GENE, I-GENE,...)\n",
    "            if word_idx != previous_word_idx:\n",
    "                if current_label.startswith(\"B-\") and previous_label == current_label:\n",
    "                    i_label = \"I-\" + current_label.split(\"-\")[1]\n",
    "                    label_ids.append(label2id[i_label])\n",
    "                else:\n",
    "                    label_ids.append(label2id[current_label])\n",
    "            else:\n",
    "                label_ids.append(label2id[current_label])\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "            previous_label = tags[word_idx]\n",
    "\n",
    "    tokenized[\"labels\"] = label_ids\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataset for put Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec5d7cb21cc48878885504ef712cf16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/189108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c502c9e6cec343e8884989bc37e800ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24314 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d91a2fedb743d6b2b2bea4ac319a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_hf_dataset(split_data):\n",
    "    hf_format = []\n",
    "    for words, tags in split_data:\n",
    "        hf_format.append({\"words\": words, \"tags\": tags})\n",
    "    return Dataset.from_list(hf_format)\n",
    "\n",
    "hf_train = convert_to_hf_dataset(train_data)\n",
    "hf_valid = convert_to_hf_dataset(valid_data)\n",
    "hf_test = convert_to_hf_dataset(test_data)\n",
    "\n",
    "# Subtokenizing(map for apply all sentence and label)\n",
    "hf_train = hf_train.map(tokenize_and_align_labels)\n",
    "hf_valid = hf_valid.map(tokenize_and_align_labels)\n",
    "hf_test = hf_test.map(tokenize_and_align_labels)\n",
    "\n",
    "# Convert Pytorch Tensor\n",
    "hf_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "hf_valid.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "hf_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaaa\\anaconda3\\envs\\genetic\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bio_ner_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    pred_labels = [[id2label[p] for p, l in zip(pred, label) if l != -100] for pred, label in zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=pred_labels, references=true_labels)\n",
    "    return {\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaaa\\AppData\\Local\\Temp\\ipykernel_22052\\359783179.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70917' max='70917' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70917/70917 2:33:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>0.999681</td>\n",
       "      <td>0.999652</td>\n",
       "      <td>0.999859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.999696</td>\n",
       "      <td>0.999764</td>\n",
       "      <td>0.999628</td>\n",
       "      <td>0.999872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.999879</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999858</td>\n",
       "      <td>0.999953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=70917, training_loss=0.002171633182793003, metrics={'train_runtime': 9237.3021, 'train_samples_per_second': 61.417, 'train_steps_per_second': 7.677, 'total_flos': 3.706098547567104e+16, 'train_loss': 0.002171633182793003, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_valid,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, id2label):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)  \n",
    "            attention_mask = batch[\"attention_mask\"].to(device)  \n",
    "            labels = batch[\"labels\"].to(device)  \n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=2)\n",
    "\n",
    "            # Transform True Label and Pred Label\n",
    "            for i in range(len(labels)):\n",
    "                true_seq = [id2label[l.item()] for l in labels[i] if l.item() != -100]\n",
    "                pred_seq = [id2label[p.item()] for p, l in zip(preds[i], labels[i]) if l.item() != -100]\n",
    "\n",
    "                true_labels.append(true_seq)\n",
    "                predictions.append(pred_seq)\n",
    "\n",
    "    # List of List to List\n",
    "    flat_true_labels = list(chain(*true_labels))\n",
    "    flat_predictions = list(chain(*predictions))\n",
    "\n",
    "    print(classification_report(flat_true_labels, flat_predictions, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-DISEASE       0.00      0.00      0.00         2\n",
      "      B-GENE       1.00      1.00      1.00    336423\n",
      "   I-DISEASE       1.00      1.00      1.00    385285\n",
      "           O       1.00      1.00      1.00    727733\n",
      "\n",
      "    accuracy                           1.00   1449443\n",
      "   macro avg       0.75      0.75      0.75   1449443\n",
      "weighted avg       1.00      1.00      1.00   1449443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, hf_test, id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('final_ner_model\\\\tokenizer_config.json',\n",
       " 'final_ner_model\\\\special_tokens_map.json',\n",
       " 'final_ner_model\\\\vocab.txt',\n",
       " 'final_ner_model\\\\added_tokens.json',\n",
       " 'final_ner_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"final_ner_model\")\n",
    "tokenizer.save_pretrained(\"final_ner_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ner(sentence):\n",
    "    tokenized_input = tokenizer(\n",
    "        sentence,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokenized_input)\n",
    "\n",
    "    # Predict\n",
    "    logits = output.logits\n",
    "    predictions = torch.argmax(logits, dim=2).squeeze().tolist()\n",
    "\n",
    "    # ID to Token\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"].squeeze().tolist())\n",
    "    predicted_labels = [model.config.id2label[p] for p in predictions]\n",
    "\n",
    "    return list(zip(tokens, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = \"final_ner_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[CLS]', 'B-GENE'), ('the', 'O'), ('br', 'B-GENE'), ('##ca', 'B-GENE'), ('##1', 'B-GENE'), ('gene', 'O'), ('is', 'O'), ('associated', 'O'), ('with', 'O'), ('breast', 'I-DISEASE'), ('cancer', 'O'), ('.', 'O'), ('[SEP]', 'B-GENE')]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(saved_model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(saved_model_path)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "text = \"The BRCA1 gene is associated with breast cancer.\"\n",
    "\n",
    "predict = predict_ner(text)\n",
    "print(predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
