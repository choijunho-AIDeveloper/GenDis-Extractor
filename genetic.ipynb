{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유전자 변이 해석\n",
    "Get Data \"wget https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz\" to CMD for NER   \n",
    "Get Data \"wget ftp://ftp.ncbi.nlm.nih.gov/pub/lu/BioRED/BIORED.zip\" to CMD for RE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfile_path = \"./variant_summary.csv\"\\ndf = pd.read_csv(file_path, sep=\"\\t\", low_memory=False)\\n\\ndf.dropna(inplace=True)\\n\\nprint(df.head())\\'\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "file_path = \"./variant_summary.csv\"\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df.head())'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Train Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Train Data Generate\\nsentences = []\\n\\n# Filter\\nFilter = [\"not specified\", \"not provided\", \"unknown\"]\\n\\n# Template\\ntemplates = [\\n    \"Mutations in the {gene} gene can cause {disease}.\",\\n    \"Variations in the {gene} gene are associated with {disease}.\",\\n    \"Abnormalities in the {gene} gene may contribute to {disease}.\",\\n    \"Specific mutations in the {gene} gene can increase the risk of {disease}.\",\\n    \"Certain variants of the {gene} gene are frequently observed in patients with {disease}.\"\\n    ]\\n\\nunique_gene_disease = set()\\nfor _, row in df.iterrows():\\n    gene = row[\"GeneSymbol\"]\\n    disease = row[\"PhenotypeList\"].split(\"|\")[0]  # \"Hereditary spastic paraplegia 48|not provided\" -> \"Hereditary spastic paraplegia 48\"\\n\\n    if disease.lower() not in Filter:\\n        unique_gene_disease.add((gene, disease))\\n\\nfor gene, disease in unique_gene_disease:\\n    for template in templates:\\n        sentence = template.format(gene = gene, disease = disease)\\n        sentences.append((sentence, gene, disease))\\n\\nfor sentence in sentences[:5]:\\n    print(sentence)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Train Data Generate\n",
    "sentences = []\n",
    "\n",
    "# Filter\n",
    "Filter = [\"not specified\", \"not provided\", \"unknown\"]\n",
    "\n",
    "# Template\n",
    "templates = [\n",
    "    \"Mutations in the {gene} gene can cause {disease}.\",\n",
    "    \"Variations in the {gene} gene are associated with {disease}.\",\n",
    "    \"Abnormalities in the {gene} gene may contribute to {disease}.\",\n",
    "    \"Specific mutations in the {gene} gene can increase the risk of {disease}.\",\n",
    "    \"Certain variants of the {gene} gene are frequently observed in patients with {disease}.\"\n",
    "    ]\n",
    "\n",
    "unique_gene_disease = set()\n",
    "for _, row in df.iterrows():\n",
    "    gene = row[\"GeneSymbol\"]\n",
    "    disease = row[\"PhenotypeList\"].split(\"|\")[0]  # \"Hereditary spastic paraplegia 48|not provided\" -> \"Hereditary spastic paraplegia 48\"\n",
    "\n",
    "    if disease.lower() not in Filter:\n",
    "        unique_gene_disease.add((gene, disease))\n",
    "\n",
    "for gene, disease in unique_gene_disease:\n",
    "    for template in templates:\n",
    "        sentence = template.format(gene = gene, disease = disease)\n",
    "        sentences.append((sentence, gene, disease))\n",
    "\n",
    "for sentence in sentences[:5]:\n",
    "    print(sentence)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef convert_to_bio(sentence, gene, disease):\\n    words = sentence.split()\\n    labels = [\"O\"] * len(words)\\n\\n    # Gene Tagging\\n    for i, word in enumerate(words):\\n        if word == gene:\\n            labels[i] = \"B-GENE\"\\n    \\n    # Disease Tagging\\n    disease_words = disease.split()\\n    for i, word in enumerate(words):\\n        if word in disease_words:\\n            labels[i] = \"B-DISEASE\" if i == 0 else \"I-DISEASE\"\\n    \\n    return list(zip(words, labels))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def convert_to_bio(sentence, gene, disease):\n",
    "    words = sentence.split()\n",
    "    labels = [\"O\"] * len(words)\n",
    "\n",
    "    # Gene Tagging\n",
    "    for i, word in enumerate(words):\n",
    "        if word == gene:\n",
    "            labels[i] = \"B-GENE\"\n",
    "    \n",
    "    # Disease Tagging\n",
    "    disease_words = disease.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word in disease_words:\n",
    "            labels[i] = \"B-DISEASE\" if i == 0 else \"I-DISEASE\"\n",
    "    \n",
    "    return list(zip(words, labels))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbio_sentences = [convert_to_bio(sentence, gene, disease) for sentence, gene, disease in sentences]\\n\\nfor bio in bio_sentences[:5]:\\n    print(bio)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bio_sentences = [convert_to_bio(sentence, gene, disease) for sentence, gene, disease in sentences]\n",
    "\n",
    "for bio in bio_sentences[:5]:\n",
    "    print(bio)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\"ner_dataset.txt\", \"w\", encoding=\"utf-8\") as f:\\n    for bio in bio_sentences:\\n        for word, tag in bio:\\n            f.write(f\"{word} {tag}\\n\")\\n        f.write(\"\\n\")\\'\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NER Train Data Save\n",
    "'''\n",
    "with open(\"ner_dataset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for bio in bio_sentences:\n",
    "        for word, tag in bio:\n",
    "            f.write(f\"{word} {tag}\\n\")\n",
    "        f.write(\"\\n\")'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\"O\" : Nothing\n",
    "\"B-GENE\" : Gene Token(Begin)\n",
    "\"I-GENE\" : Gene Token(Inside)\n",
    "\"B-DISEASE\" : Disease Token(Begin)\n",
    "\"I-DISEASE\" : Disease Token(Inside)\n",
    "'''\n",
    "\n",
    "label_list = [\"O\", \"B-GENE\", \"I-GENE\", \"B-DISEASE\", \"I-DISEASE\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence combine\n",
    "def load_ner_data(file_path):\n",
    "    # Restore Sentence\n",
    "    sentences = []\n",
    "    # Words in Sentence\n",
    "    words = []\n",
    "    # Tag in Sentence\n",
    "    tags = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # config \"\"(Finish Sentence)\n",
    "            if not line:\n",
    "                if words:\n",
    "                    sentences.append((words, tags))\n",
    "                    words = []\n",
    "                    tags = []\n",
    "            else:\n",
    "                token, label = line.split()\n",
    "                words.append(token)\n",
    "                tags.append(label)\n",
    "        # If Last Sentence not Finish with \"\" Preprocess\n",
    "        if words:\n",
    "            sentences.append((words, tags))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./ner_dataset.txt\"\n",
    "dataset = load_ner_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Valid, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : valid :  test : 189108 : 24314 : 56733\n"
     ]
    }
   ],
   "source": [
    "# Split train : valid : test = 7 : 1 : 2\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.7, random_state=42)\n",
    "\n",
    "print(f\"train : valid :  test : {len(train_data)} : {len(valid_data)} : {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    words = examples['words']\n",
    "    tags = examples['tags']\n",
    "    \n",
    "    # Word to SubWord(Tokenizing)\n",
    "    tokenized = tokenizer(\n",
    "        words,\n",
    "        truncation=True,\n",
    "        padding = \"max_length\",\n",
    "        max_length = 128,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    # Subword Match Origin Word Label\n",
    "    label_ids = []\n",
    "    word_ids = tokenized.word_ids(batch_index=0)\n",
    "\n",
    "    previous_word_idx = None\n",
    "    previous_label = \"O\"\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        # If token [CLS], [SEP], [PAD], ignore\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        else:\n",
    "            current_label = tags[word_idx]\n",
    "            \n",
    "            # If Subword seperate GENE, label -> (B-GENE, I-GENE,...)\n",
    "            if word_idx != previous_word_idx:\n",
    "                if current_label.startswith(\"B-\") and previous_label == current_label:\n",
    "                    i_label = \"I-\" + current_label.split(\"-\")[1]\n",
    "                    label_ids.append(label2id[i_label])\n",
    "                else:\n",
    "                    label_ids.append(label2id[current_label])\n",
    "            else:\n",
    "                label_ids.append(label2id[current_label])\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "            previous_label = tags[word_idx]\n",
    "\n",
    "    tokenized[\"labels\"] = label_ids\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataset for put Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa00a94f86648338f8fabd6b7f19df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/189108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7ded6fa03a473eb31855896b581e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24314 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e389987b1c0425e889744c9c210ab01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_hf_dataset(split_data):\n",
    "    hf_format = []\n",
    "    for words, tags in split_data:\n",
    "        hf_format.append({\"words\": words, \"tags\": tags})\n",
    "    return Dataset.from_list(hf_format)\n",
    "\n",
    "hf_train = convert_to_hf_dataset(train_data)\n",
    "hf_valid = convert_to_hf_dataset(valid_data)\n",
    "hf_test = convert_to_hf_dataset(test_data)\n",
    "\n",
    "# Subtokenizing(map for apply all sentence and label)\n",
    "hf_train = hf_train.map(tokenize_and_align_labels)\n",
    "hf_valid = hf_valid.map(tokenize_and_align_labels)\n",
    "hf_test = hf_test.map(tokenize_and_align_labels)\n",
    "\n",
    "# Convert Pytorch Tensor\n",
    "hf_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "hf_valid.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "hf_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaaa\\anaconda3\\envs\\genetic\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bio_ner_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    pred_labels = [[id2label[p] for p, l in zip(pred, label) if l != -100] for pred, label in zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=pred_labels, references=true_labels)\n",
    "    return {\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaaa\\AppData\\Local\\Temp\\ipykernel_1424\\1558155240.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_valid,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, id2label):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)  \n",
    "            attention_mask = batch[\"attention_mask\"].to(device)  \n",
    "            labels = batch[\"labels\"].to(device)  \n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=2)\n",
    "\n",
    "            # Transform True Label and Pred Label\n",
    "            for i in range(len(labels)):\n",
    "                true_seq = [id2label[l.item()] for l in labels[i] if l.item() != -100]\n",
    "                pred_seq = [id2label[p.item()] for p, l in zip(preds[i], labels[i]) if l.item() != -100]\n",
    "\n",
    "                true_labels.append(true_seq)\n",
    "                predictions.append(pred_seq)\n",
    "\n",
    "    # List of List to List\n",
    "    flat_true_labels = list(chain(*true_labels))\n",
    "    flat_predictions = list(chain(*predictions))\n",
    "\n",
    "    print(classification_report(flat_true_labels, flat_predictions, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model(model, hf_test, id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"final_ner_model\")\n",
    "# tokenizer.save_pretrained(\"final_ner_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ner(sentence):\n",
    "    tokenized_input = tokenizer(\n",
    "        sentence,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokenized_input)\n",
    "\n",
    "    # Predict\n",
    "    logits = output.logits\n",
    "    predictions = torch.argmax(logits, dim=2).squeeze().tolist()\n",
    "\n",
    "    # ID to Token\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"].squeeze().tolist())\n",
    "    predicted_labels = [model.config.id2label[p] for p in predictions]\n",
    "\n",
    "    return list(zip(tokens, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = \"final_ner_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[CLS]', 'B-GENE'), ('the', 'O'), ('br', 'B-GENE'), ('##ca', 'B-GENE'), ('##1', 'B-GENE'), ('gene', 'O'), ('is', 'O'), ('associated', 'O'), ('with', 'O'), ('breast', 'I-DISEASE'), ('cancer', 'O'), ('.', 'O'), ('[SEP]', 'B-GENE')]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(saved_model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(saved_model_path)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "text = \"The BRCA1 gene is associated with breast cancer.\"\n",
    "\n",
    "predict = predict_ner(text)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract NER Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'GENE', 'text': '[CLS]'}, {'label': 'GENE', 'text': 'br'}, {'label': 'GENE', 'text': '##ca'}, {'label': 'GENE', 'text': '##1'}, {'label': 'GENE', 'text': '[SEP]'}]\n"
     ]
    }
   ],
   "source": [
    "ner_result = predict\n",
    "entities = []\n",
    "current_entity = None\n",
    "for token, label in ner_result:\n",
    "    if label.startswith('B-'):\n",
    "        if current_entity:\n",
    "            entities.append(current_entity)\n",
    "            current_entity = None\n",
    "        current_entity = {'label': label[2:], 'text': token}\n",
    "    elif label.startswith('I-') and current_entity and current_entity['label'] == label[2:]:\n",
    "        current_entity['text'] += token.replace(\"##\", \"\")\n",
    "    else:\n",
    "        if current_entity:\n",
    "            entities.append(current_entity)\n",
    "            current_entity = None\n",
    "\n",
    "if current_entity:\n",
    "    entities.append(current_entity)\n",
    "\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'br', '##ca', '##1', '[SEP]']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene = [entity['text'] for entity in entities if entity['label'] == 'GENE']\n",
    "disease = [entity['text'] for entity in entities if entity['label'] == 'DISEASE']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
