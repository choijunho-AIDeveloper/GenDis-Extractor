{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유전자 변이 해석\n",
    "Get Data \"wget https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz\" to CMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   #AlleleID                       Type  \\\n",
      "0      15041                      Indel   \n",
      "1      15041                      Indel   \n",
      "2      15042                   Deletion   \n",
      "3      15042                   Deletion   \n",
      "4      15043  single nucleotide variant   \n",
      "\n",
      "                                                Name  GeneID GeneSymbol  \\\n",
      "0  NM_014855.3(AP5Z1):c.80_83delinsTGCTGTAAACTGTA...    9907      AP5Z1   \n",
      "1  NM_014855.3(AP5Z1):c.80_83delinsTGCTGTAAACTGTA...    9907      AP5Z1   \n",
      "2     NM_014855.3(AP5Z1):c.1413_1426del (p.Leu473fs)    9907      AP5Z1   \n",
      "3     NM_014855.3(AP5Z1):c.1413_1426del (p.Leu473fs)    9907      AP5Z1   \n",
      "4       NM_014630.3(ZNF592):c.3136G>A (p.Gly1046Arg)    9640     ZNF592   \n",
      "\n",
      "      HGNC_ID    ClinicalSignificance  ClinSigSimple LastEvaluated  \\\n",
      "0  HGNC:22197              Pathogenic              1  Jun 25, 2024   \n",
      "1  HGNC:22197              Pathogenic              1  Jun 25, 2024   \n",
      "2  HGNC:22197              Pathogenic              1  Jun 29, 2010   \n",
      "3  HGNC:22197              Pathogenic              1  Jun 29, 2010   \n",
      "4  HGNC:28986  Uncertain significance              0  Jun 29, 2015   \n",
      "\n",
      "   RS# (dbSNP)  ...      AlternateAlleleVCF SomaticClinicalImpact  \\\n",
      "0    397704705  ...  TGCTGTAAACTGTAACTGTAAA                     -   \n",
      "1    397704705  ...  TGCTGTAAACTGTAACTGTAAA                     -   \n",
      "2    397704709  ...                       G                     -   \n",
      "3    397704709  ...                       G                     -   \n",
      "4    150829393  ...                       A                     -   \n",
      "\n",
      "  SomaticClinicalImpactLastEvaluated ReviewStatusClinicalImpact Oncogenicity  \\\n",
      "0                                  -                          -            -   \n",
      "1                                  -                          -            -   \n",
      "2                                  -                          -            -   \n",
      "3                                  -                          -            -   \n",
      "4                                  -                          -            -   \n",
      "\n",
      "  OncogenicityLastEvaluated ReviewStatusOncogenicity  \\\n",
      "0                         -                        -   \n",
      "1                         -                        -   \n",
      "2                         -                        -   \n",
      "3                         -                        -   \n",
      "4                         -                        -   \n",
      "\n",
      "  SCVsForAggregateGermlineClassification  \\\n",
      "0              SCV001451119|SCV005622007   \n",
      "1              SCV001451119|SCV005622007   \n",
      "2                           SCV000020156   \n",
      "3                           SCV000020156   \n",
      "4                           SCV000020157   \n",
      "\n",
      "  SCVsForAggregateSomaticClinicalImpact  \\\n",
      "0                                     -   \n",
      "1                                     -   \n",
      "2                                     -   \n",
      "3                                     -   \n",
      "4                                     -   \n",
      "\n",
      "   SCVsForAggregateOncogenicityClassification  \n",
      "0                                           -  \n",
      "1                                           -  \n",
      "2                                           -  \n",
      "3                                           -  \n",
      "4                                           -  \n",
      "\n",
      "[5 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./variant_summary.csv\"\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Train Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mutations in the CRYBA1 gene can cause Inborn genetic diseases.', 'CRYBA1', 'Inborn genetic diseases')\n",
      "('Variations in the CRYBA1 gene are associated with Inborn genetic diseases.', 'CRYBA1', 'Inborn genetic diseases')\n",
      "('Abnormalities in the CRYBA1 gene may contribute to Inborn genetic diseases.', 'CRYBA1', 'Inborn genetic diseases')\n",
      "('Specific mutations in the CRYBA1 gene can increase the risk of Inborn genetic diseases.', 'CRYBA1', 'Inborn genetic diseases')\n",
      "('Certain variants of the CRYBA1 gene are frequently observed in patients with Inborn genetic diseases.', 'CRYBA1', 'Inborn genetic diseases')\n"
     ]
    }
   ],
   "source": [
    "# Train Data Generate\n",
    "sentences = []\n",
    "\n",
    "# Filter\n",
    "Filter = [\"not specified\", \"not provided\", \"unknown\"]\n",
    "\n",
    "# Template\n",
    "templates = [\n",
    "    \"Mutations in the {gene} gene can cause {disease}.\",\n",
    "    \"Variations in the {gene} gene are associated with {disease}.\",\n",
    "    \"Abnormalities in the {gene} gene may contribute to {disease}.\",\n",
    "    \"Specific mutations in the {gene} gene can increase the risk of {disease}.\",\n",
    "    \"Certain variants of the {gene} gene are frequently observed in patients with {disease}.\"\n",
    "    ]\n",
    "\n",
    "unique_gene_disease = set()\n",
    "for _, row in df.iterrows():\n",
    "    gene = row[\"GeneSymbol\"]\n",
    "    disease = row[\"PhenotypeList\"].split(\"|\")[0]  # \"Hereditary spastic paraplegia 48|not provided\" -> \"Hereditary spastic paraplegia 48\"\n",
    "\n",
    "    if disease.lower() not in Filter:\n",
    "        unique_gene_disease.add((gene, disease))\n",
    "\n",
    "for gene, disease in unique_gene_disease:\n",
    "    for template in templates:\n",
    "        sentence = template.format(gene = gene, disease = disease)\n",
    "        sentences.append((sentence, gene, disease))\n",
    "\n",
    "for sentence in sentences[:5]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio(sentence, gene, disease):\n",
    "    words = sentence.split()\n",
    "    labels = [\"O\"] * len(words)\n",
    "\n",
    "    # Gene Tagging\n",
    "    for i, word in enumerate(words):\n",
    "        if word == gene:\n",
    "            labels[i] = \"B-GENE\"\n",
    "    \n",
    "    # Disease Tagging\n",
    "    disease_words = disease.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word in disease_words:\n",
    "            labels[i] = \"B-DISEASE\" if i == 0 else \"I-DISEASE\"\n",
    "    \n",
    "    return list(zip(words, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mutations', 'O'), ('in', 'O'), ('the', 'O'), ('CRYBA1', 'B-GENE'), ('gene', 'O'), ('can', 'O'), ('cause', 'O'), ('Inborn', 'I-DISEASE'), ('genetic', 'I-DISEASE'), ('diseases.', 'O')]\n",
      "[('Variations', 'O'), ('in', 'O'), ('the', 'O'), ('CRYBA1', 'B-GENE'), ('gene', 'O'), ('are', 'O'), ('associated', 'O'), ('with', 'O'), ('Inborn', 'I-DISEASE'), ('genetic', 'I-DISEASE'), ('diseases.', 'O')]\n",
      "[('Abnormalities', 'O'), ('in', 'O'), ('the', 'O'), ('CRYBA1', 'B-GENE'), ('gene', 'O'), ('may', 'O'), ('contribute', 'O'), ('to', 'O'), ('Inborn', 'I-DISEASE'), ('genetic', 'I-DISEASE'), ('diseases.', 'O')]\n",
      "[('Specific', 'O'), ('mutations', 'O'), ('in', 'O'), ('the', 'O'), ('CRYBA1', 'B-GENE'), ('gene', 'O'), ('can', 'O'), ('increase', 'O'), ('the', 'O'), ('risk', 'O'), ('of', 'O'), ('Inborn', 'I-DISEASE'), ('genetic', 'I-DISEASE'), ('diseases.', 'O')]\n",
      "[('Certain', 'O'), ('variants', 'O'), ('of', 'O'), ('the', 'O'), ('CRYBA1', 'B-GENE'), ('gene', 'O'), ('are', 'O'), ('frequently', 'O'), ('observed', 'O'), ('in', 'O'), ('patients', 'O'), ('with', 'O'), ('Inborn', 'I-DISEASE'), ('genetic', 'I-DISEASE'), ('diseases.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "bio_sentences = [convert_to_bio(sentence, gene, disease) for sentence, gene, disease in sentences]\n",
    "\n",
    "for bio in bio_sentences[:5]:\n",
    "    print(bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER Train Data Save\n",
    "with open(\"ner_dataset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for bio in bio_sentences:\n",
    "        for word, tag in bio:\n",
    "            f.write(f\"{word} {tag}\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\"O\" : Nothing\n",
    "\"B-GENE\" : Gene Token(Begin)\n",
    "\"I-GENE\" : Gene Token(Inside)\n",
    "\"B-DISEASE\" : Disease Token(Begin)\n",
    "\"I-DISEASE\" : Disease Token(Inside)\n",
    "'''\n",
    "\n",
    "label_list = [\"O\", \"B-GENE\", \"I-GENE\", \"B-DISEASE\", \"I-DISEASE\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence combine\n",
    "def load_ner_data(file_path):\n",
    "    # Restore Sentence\n",
    "    sentences = []\n",
    "    # Words in Sentence\n",
    "    words = []\n",
    "    # Tag in Sentence\n",
    "    tags = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # config \"\"(Finish Sentence)\n",
    "            if not line:\n",
    "                if words:\n",
    "                    sentences.append((words, tags))\n",
    "                    words = []\n",
    "                    tags = []\n",
    "            else:\n",
    "                token, label = line.split()\n",
    "                words.append(token)\n",
    "                tags.append(label)\n",
    "        # If Last Sentence not Finish with \"\" Preprocess\n",
    "        if words:\n",
    "            sentences.append((words, tags))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./ner_dataset.txt\"\n",
    "dataset = load_ner_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Valid, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : valid :  test : 189108 : 24314 : 56733\n"
     ]
    }
   ],
   "source": [
    "# Split train : valid : test = 7 : 1 : 2\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.7, random_state=42)\n",
    "\n",
    "print(f\"train : valid :  test : {len(train_data)} : {len(valid_data)} : {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    words, tags = examples\n",
    "    \n",
    "    # Word to SubWord(Tokenizing)\n",
    "    tokenized = tokenizer(\n",
    "        words,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    # Subword Match Origin Word Label\n",
    "    label_ids = []\n",
    "    word_ids = tokenized.word_ids(batch_index=0)\n",
    "\n",
    "    previous_word_idx = None\n",
    "    previous_label = \"O\"\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        # If token [CLS], [SEP], [PAD], ignore\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        else:\n",
    "            current_label = tags[word_idx]\n",
    "            \n",
    "            # If Subword seperate GENE, label -> (B-GENE, I-GENE,...)\n",
    "            if word_idx != previous_word_idx:\n",
    "                if current_label.startswith(\"B-\") and previous_label == current_label:\n",
    "                    i_label = \"I-\" + current_label.split(\"-\")[1]\n",
    "                    label_ids.append(label2id[i_label])\n",
    "                else:\n",
    "                    label_ids.append(label2id[current_label])\n",
    "            else:\n",
    "                label_ids.append(label2id[current_label])\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "            previous_label = tags[word_idx]\n",
    "\n",
    "    tokenized[\"labels\"] = label_ids\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataset for put Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a6544fa7054dd5955d6e8db4bb7c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/189108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m hf_test = convert_to_hf_dataset(test_data)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Subtokenizing(map for apply all sentence and label)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m hf_train = \u001b[43mhf_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_and_align_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m hf_valid = hf_valid.map(tokenize_and_align_labels, batched = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     14\u001b[39m hf_test = hf_test.map(tokenize_and_align_labels, batched = \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aaaa\\anaconda3\\envs\\genetic\\Lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: List[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aaaa\\anaconda3\\envs\\genetic\\Lib\\site-packages\\datasets\\arrow_dataset.py:3079\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[39m\n\u001b[32m   3073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3074\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3075\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3076\u001b[39m         total=pbar_total,\n\u001b[32m   3077\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3078\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aaaa\\anaconda3\\envs\\genetic\\Lib\\site-packages\\datasets\\arrow_dataset.py:3519\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[39m\n\u001b[32m   3517\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3518\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3519\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3521\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aaaa\\anaconda3\\envs\\genetic\\Lib\\site-packages\\datasets\\arrow_dataset.py:3469\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3467\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3468\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3469\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aaaa\\anaconda3\\envs\\genetic\\Lib\\site-packages\\datasets\\arrow_dataset.py:3392\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3390\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3391\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3392\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtokenize_and_align_labels\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m     22\u001b[39m     label_ids.append([-\u001b[32m100\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(tokenized_inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m]))\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword_ids\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If token [CLS], [SEP], [PAD], ignore\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_list\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "def convert_to_hf_dataset(split_data):\n",
    "    hf_format = []\n",
    "    for words, tags in split_data:\n",
    "        hf_format.append({\"words\": words, \"tags\": tags})\n",
    "    return Dataset.from_list(hf_format)\n",
    "\n",
    "hf_train = convert_to_hf_dataset(train_data)\n",
    "hf_valid = convert_to_hf_dataset(valid_data)\n",
    "hf_test = convert_to_hf_dataset(test_data)\n",
    "\n",
    "# Subtokenizing(map for apply all sentence and label)\n",
    "hf_train = hf_train.map(tokenize_and_align_labels, batched = True)\n",
    "hf_valid = hf_valid.map(tokenize_and_align_labels, batched = True)\n",
    "hf_test = hf_test.map(tokenize_and_align_labels, batched = True)\n",
    "\n",
    "# Convert Pytorch Tensor\n",
    "hf_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "hf_valid.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "hf_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bio_ner_model\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    pred_labels = [[id2label[p] for p, l in zip(pred, label) if l != -100] for pred, label in zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=pred_labels, references=true_labels)\n",
    "    return {\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_valid,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=2)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                true_labels.append([id2label[l.item()] for l in labels[i] if l.item() != -100])\n",
    "                predictions.append([id2label[p.item()] for p, l in zip(preds[i], labels[i]) if l.item() != -100])\n",
    "\n",
    "    print(classification_report(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, hf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"final_ner_model\")\n",
    "tokenizer.save_pretrained(\"final_ner_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ner(sentence):\n",
    "    # Input sentence convert token\n",
    "    tokenized_input = tokenizer(\n",
    "        sentence,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokenized_input)\n",
    "\n",
    "    # Return Max Predict Value\n",
    "    logits = output.logits\n",
    "    predictions = torch.argmax(logits, dim=2).squeeze().tolist()\n",
    "\n",
    "    # Convert Id to Origin Token\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"].squeeze().tolist())\n",
    "\n",
    "    predicted_labels = [model.config.id2label[p] for p in predictions]\n",
    "\n",
    "    # for token, label in zip(tokens, predicted_labels):\n",
    "    #     print(f\"{token:15} --> {label}\")\n",
    "\n",
    "    return list(zip(tokens, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = \"final_ner_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(saved_model_path)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "text = \"The BRCA1 gene is associated with breast cancer.\"\n",
    "\n",
    "predict = predict_ner(text)\n",
    "print(predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
